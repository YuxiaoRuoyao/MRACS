---
title: "mrScan_vignette"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{mrScan_vignette}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```


```{r setup}
library(mrScan)
library(dplyr)
library(TwoSampleMR)
library(mr.raps)
```

We'll use C-reactive protein (CRP) level (ieu-b-35) as the exposure and 
schizophrenia (ieu-b-42) as the outcome in this example. Please note if you want 
to conduct the whole analysis parallel, we provide a Snakemake pipeline and you 
can check the usage in the README.

```{r}
x_id <- "ieu-b-35"
y_id <- "ieu-b-42"
```

## Step 1: Initially extract trait list

We only search traits in ukb-b batch (IEU analysis of UK Biobank phenotypes) in IEU 
OpenGWAS database in this example. You can add more batches according to your needs. 
You can check the list of data batches in IEU GWAS database by ieugwasr::batches().  


If you want to use local data for the main exposure, set `type_exposure="local"`, 
and input data path in `file_path`. You need to download the LD reference files 
[here](http://fileserve.mrcieu.ac.uk/ld/1kg.v3.tgz) to perform LD clumping. And then 
input `ref_path` to your directory path of LD reference file. You can do this like:

```{r eval=FALSE}
extract_traits(id_exposure = x_id,type_exposure = "local",
               file_path = "/path/to/ieu-b-35.vcf",
               ref_path = "/path/to/reference/EUR")
```

If you already have a list of candidate traits and don't want to search traits in 
IEU OpenGWAS database, you can download data locally. You need to specify the file path 
for each trait in `file_list` and trait ID in `trait_list`. Then check the data to find 
column names of `SNP` (rsid), `beta_hat`, `se`, `p_value` and input in the function like: 
```{r eval=FALSE}
extract_traits(id_exposure = x_id,type_candidate_traits = "local",
               file_list = c("/path/to/trait1","/path/to/trait2"),
               trait_list = c("trait1","trait2"),
               snp_name_list = c("snp_name1","snp_name2"),
               beta_hat_name_list = c("beta_hat_name1","beta_hat_name2"),
               se_name_list = c("se_name1","se_name2"),
               p_value_name_list = c("pval_name1","pval_name2"))
```

Note the `p_value_name_list` could be NA if the data does not have pvalue. 
If the local files are downloaded from IEU Open GWAS database or EBI database (harmonized one), 
you can just omit these parameters and don???t need to check the exact column names. 
But you must need to provide snp, beta_hat and se column names if it???s a flat file provided by yourself.
You can also create a candidate trait info dataframe for that. (Please check the package 
README, Optional Step, for detailed instruction). Assume the info dataframe is `df_info`,
you can do this by:
```{r eval=FALSE}
extract_traits(id_exposure = x_id,type_candidate_traits = "local",
               file_list = df_info$path,trait_list = df_info$trait_ID,
               snp_name_list = df_info$snp,beta_hat_name_list = df_info$beta_hat,
               se_name_list = df_info$se,p_value_name_list = df_info$p_value)
```


In this analysis, we get 63 traits having at least 5 shared variants with the CRP-level.

```{r eval=FALSE}
res_step1 <- extract_traits(id_exposure = x_id,batch = c("ukb-b"))
```


```{r}
load("data/res_step1.rda")
length(res_step1$id.list)
head(res_step1$trait.info)
```

## Step 2: Quality control 

After initial filtering, we get 62 traits left. We just select traits for both gender, 
European and with the number of SNPs > 1e6. You can change the filtering standard 
by your need. You can check the info matrix for the status of each trait. 
```{r eval=FALSE}
res_step2 <- quality_control(dat = res_step1$trait.info)
```

It deletes a menarche trait which should be a female trait, but the sex information 
is "Males and Females".
```{r}
load("data/res_step2.rda")
length(res_step2$id.list)
head(res_step2$trait.info)
res_step2$trait.info %>% filter(status == "delete in QC")
```


## Step 3: Downstream traits filtering

Downstream traits of both main exposure and each outcome should be deleted since 
the precision for estimating direct causal effect of the main exposure to the outcome 
could be decreased if too many genetic instruments of relationship between the main 
exposure and downstream traits were included. In this step, we will do bidirection 
MR estimates between traits and X/Y and then select all upstream traits and exclude 
downstream traits in this step.  

### Bidirection MR

First of all, we'll do simple MR between each candidate traits and either the main 
exposure and the outcome. To be specific, we will do four MR estimates including:
$X \rightarrow Z$, $Z \rightarrow X$, $Y \rightarrow Z$ and $Z \rightarrow Y$. 
The default method is `MR-RAPs` with `over.dispersion=TRUE`, `loss.function="tukey"`, 
and you can change parameters or the method.  


Note that this step may take some time. We recommend you to extract instruments 
for all traits first to avoid repeating extracting instruments.

```{r eval=FALSE}
# extract instruments for all traits
all_inst <- extract_instruments(c(x_id,y_id,res_step2$id.list))
df_pair <- expand.grid(trait1 = c(x_id,y_id), trait2 = res_step2$id.list)
res_mr <- purrr::map2(df_pair$trait1[1:5], df_pair$trait2[1:5], function(i, j){
  ex_dat1 <- all_inst %>% filter(id.exposure == i)
  ex_dat2 <- all_inst %>% filter(id.exposure == j)
  bidirection_mr(ex_dat1 = ex_dat1, ex_dat2 = ex_dat2)
})
```

We want to delete traits with limited number of instruments. The default `min_instruments = 3`.
```{r}
empty_id <- df_pair[which(sapply(res_mr,is.null)),"trait2"] %>% unique() %>% as.vector()
res_step2$trait.info[res_step2$trait.info$id %in% empty_id,"status"] <- "delete due to not enough instruments"
res_step2$id.list <- res_step2$id.list[!res_step2$id.list %in% empty_id]
```

### Downstream filtering

We'll do one-sided t-test based on the results of bidirection MR. The default t-test 
cutoff is 0.05 and if you want to include more traits, you can loose the cutoff 
to 0.1 or more. We select all upstream traits for either X or Y and delete all 
downstream traits for either the main exposure and the outcome.

After this step, we select 40 traits. The output contains select trait list, 
updated trait into matrix, and all input traits with bidirection estimates, 
t-test results. 
```{r}
res_mr_filter <- lapply(res_mr, function(x) x[lengths(x) > 0])
all_mr <- do.call(Map, c(f = rbind, res_mr_filter))
res_downstream <- downstream_filter(id_exposure = x_id,id.list = res_step2$id.list,
                                    df_info = res_step2$trait.info, res = all_mr)
```


```{r}
load("../data/res_step3.rda")
length(res_step3$id.list)
head(res_step3$trait.info)
head(res_step3$df_bidirection)
# See selected traits
res_step3$trait.info %>% filter(id %in% res_step3$id.list)
```


## Step 4: Get unique traits

By the trait list above, we can see several similar or even duplicated traits are extracted. 
In this step, we'll calculate genetic correlation for each trait pair and select unique 
trait in each cluster.   

There are two ways to get trait correlation: 
1. Simply calculate string similarity by Jaro???Winkler distance. This way 
is quick but biased because it only based on provided trait names. If you want 
to quickly check it by this method, we recommend you to check the df_pair matrix 
and the cluster result.  

2. Calculate accurate genetic correlation by LDSC method. It will need you to download 
GWAS summary data locally and we recommend to do it on the server. You can follow 
the Snakemake pipeline for this step. 


The default clustering method is greedy clustering on the pairwise 
correlation matrix by the $R^2$ cutoff. You can also choose sample_size or nsnps method, 
which are basically selected traits with higher sample size or the number of SNPs. 
The following code example is for string similarity method.
```{r}
res_cor <- string_sim(id.list = res_step3$id.list, df_info = res_step3$trait.info)
res_step4 <- unique_traits(id.list = res_step3$id.list, df_info = res_step3$trait.info,
                           R_matrix = res_cor$R_matrix, df_pair = res_cor$df_pair,
                           R2_cutoff = 0.9, method = "cluster")
```


```{r}
length(res_step4$id.list)
res_step4$trait.info %>% filter(id %in% res_step4$id.list) %>% select(id,trait,cluster)
```



## Step 5: Confounder selection

Next, we will use different methods to do confounder selection. The potential options 
include classic Lasso, double classic Lasso, corrected Lasso, double corrected Lasso, 
marginal selection and etc. In this example, we will try different methods.
```{r}
res_step5_lasso <- confounder_selection(id_exposure = "ieu-b-35",id_outcome = "ieu-b-42",
                     id.list = res_step4$id.list, df_info = res_step4$trait.info,
                     method = "classic_Lasso", lambda_type = "min")
```


```{r}
res_step5_double_lasso <- confounder_selection(id_exposure = "ieu-b-35",id_outcome = "ieu-b-42",
                                               id.list = res_step4$id.list, df_info = res_step4$trait.info,
                                               method = "double_Lasso", lambda_type = "1se")
res_step5_double_lasso$id.list 
```


```{r}
res_step5_marginal <- confounder_selection(id_exposure = "ieu-b-35",id_outcome = "ieu-b-42",
                                           id.list = res_step4$id.list, df_info = res_step4$trait.info,
                                           method = "marginal",df_bidirection = res_step3$df_bidirection)
res_step5_marginal$id.list
```


```{r}
res_step5_stepwise <- confounder_selection(id_exposure = "ieu-b-35",id_outcome = "ieu-b-42",
                     id.list = res_step4$id.list, df_info = res_step4$trait.info,
                     method = "stepwise", stepwise_method = "forward")
res_step5_stepwise$id.list
```


```{r}
res_step5_correct <- confounder_selection(id_exposure = "ieu-b-35",id_outcome = "ieu-b-42",
                     id.list = res_step4$id.list, df_info = res_step4$trait.info,
                     method = "corrected_Lasso", radius_type="1se")
res_step5_correct$id.list
```


```{r}
res_step5_double_correct <- confounder_selection(id_exposure = "ieu-b-35",id_outcome = "ieu-b-42",
                     id.list = res_step4$id.list, df_info = res_step4$trait.info,
                     method = "double_corrected_Lasso", radius_type="1se")
res_step5_double_correct$id.list
```

## Step 6: MVMR analysis to get causal estimates without correlation matrix between traits

Finally, after adjusting for selected traits, we will apply different MVMR methods 
to get causal estimates from the main exposure to the outcome. The function offers 
multiple MVMR methods including regular MV-IVW, MV-median and more robust methods 
like GRAPPLE, MRBEE. In this step, we assume traits are independent with each other. 
Here we use traits selected from double corrected Lasso as the example.

```{r}
res_final_ivw <- MVMR_analysis(id_exposure = "ieu-b-35",id_outcome = "ieu-b-42",
              id.list = res_step5_double_correct$id.list,
              df_info = res_step5_double_correct$trait.info,
              MVMR_method = "IVW")
res_final_ivw_T <- MVMR_analysis(id_exposure = "ieu-b-35",id_outcome = "ieu-b-42",
              id.list = res_step5_double_correct$id.list,
              df_info = res_step5_double_correct$trait.info,
              MVMR_method = "IVW_instrument_specific")
res_final_bee <- MVMR_analysis(id_exposure = "ieu-b-35",id_outcome = "ieu-b-42",
              id.list = res_step5_double_correct$id.list,
              df_info = res_step5_double_correct$trait.info,
              MVMR_method = "MRBEE",pleio_p_thresh = 0)
res_final_grapple <- MVMR_analysis(id_exposure = "ieu-b-35",id_outcome = "ieu-b-42",
              id.list = res_step5_double_correct$id.list,
              df_info = res_step5_double_correct$trait.info,
              MVMR_method = "GRAPPLE")

```


Get direct causal estimates of CRP-level on schizophrenia by different methods:
```{r}
rbind(res_final_ivw,res_final_ivw_T) %>% dplyr::select(id.exposure,b,se,pval,method) %>%
  rename("exposure" = "id.exposure","pvalue" = "pval") %>%
  rbind(res_final_bee,res_final_grapple) %>%
  filter(exposure == "ieu-b-35")
```

## Step 7: MVMR analysis to get causal estimates with correlation matrix between traits

In order to take genetic correlation between traits into consideration, we need to 
estimate it first. You need to download raw GWAS summary statistics to local. You 
can save files on the server since they could be pretty large. 

### 1. Download GWAS summary statistics

You can use the following code to generate download.sh file and conduct it in the 
linux command line. The default position to save this file is your current working 
directory and you can change it by position parameter.
```{r}
download_gwas(id_list = c("ieu-b-35","ieu-b-42",
                          res_step5_double_correct$id.list))
```


```{bash, eval=FALSE, engine="sh"}
bash download.sh
```

### 2. Formatting and harmonize data

After downloading data, next step is to format raw GWAS summary data to remove 
ambiguous SNPs. The following code uses ieu-b-42, ieu-b-35 and ieu-b-104 as the example, and you can 
format each trait by the same rule. Put the outcome as the first trait.
```{r eval=FALSE}
library(stringr)
library(readr)
library(VariantAnnotation)
library(gwasvcf)
library(dplyr)
library(magrittr)
library(rlang)
library(purrr)
format_combine_gwas(id_list = c("ieu-b-42","ieu-b-35","ukb-b-19953"),
               file_list = c("ieu-b-42.vcf.gz","ieu-b-35.vcf.gz","ukb-b-19953.vcf.gz"),
               out_dir="data/",prefix="test")
```

### 3. LD clumping

You can download the LD reference dataset from here http://fileserve.mrcieu.ac.uk/ld/1kg.v3.tgz
This contains an LD reference panel for each of the 5 super-populations in the 1000 genomes reference dataset.
```{r}
ld_prune_chrom_plink(beta_dir = "data/",prefix = "test",ref_path = "/path/LD_reference/EUR",
                     out_dir = "/path/ld_prune_output/")
```

### 4. Calculate R correlation matrix

There are two options for this. One is by using pvalue method and the other is LDSC way. 
pvalue method is quick and easier but the estimated matrix could be biased. If you want to 
use more accurate LDSC way, you need to download reference LD score first. 

```{r eval=FALSE}
estimate_R_pval(ld_prune_file_dir = "/path/ld_prune_output/",prefix = "test",
                out_dir = "data/")
```


```{r eval=FALSE}
ldsc_full(l2_dir = "/path/l2_dir/",beta_dir = "data/",prefix = "test",out_dir = "data/")
```


```{r}
test.R_est_ldsc <- readRDS("data/test.R_est_ldsc.RDS")
test.R_est_pval <- readRDS("data/test.R_est_pval.RDS")
test.R_est_ldsc
test.R_est_pval
```


### 5. Get MVMR estimates with R matrix
```{r}
res_grapple_R <- MVMR_analysis_local(ld_prune_file_dir = "/path/ld_prune_output/",
                    prefix = "test",R = test.R_est_ldsc,
                    p_thresh = 1e-5,MVMR_method = "GRAPPLE")
res_bee_R <- MVMR_analysis_local(ld_prune_file_dir = "/path/ld_prune_output/",
                    prefix = "test",R = test.R_est_ldsc,
                    p_thresh = 5e-8, pleio_p_thresh = 0,
                    MVMR_method = "MRBEE")
res_ivw <- MVMR_analysis_local(ld_prune_file_dir = "/path/ld_prune_output/",
                    prefix = "test",p_thresh = 5e-8,
                    MVMR_method = "IVW")
```


