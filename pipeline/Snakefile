# Snakemake pipeline for running mrScan package
#
#
# LICENSE: CC0. Do what you want with the code, but it has no guarantees.
#          https://creativecommons.org/share-your-work/public-domain/cc0/
#
#
#
# ./run_snakemake.sh
#
# don't forget to update cluster.yaml


import pandas as pd
import random
import string
import os.path
from snakemake.utils import validate

###### Load configuration file
configfile: "config.yaml"

# output options
data_dir = config["out"]["data_dir"] #where the data is
out_dir = config["out"]["output_dir"] #where results will go
original_gwas_dir = config["out"]["gwas_dir"] # where originally download gwas data lives
formatted_gwas_dir = config["out"]["formatted_gwas_dir"] # where formatted gwas data lives

prefix = config["input"]["label"] + "_"
exposure_prefix = config["input"]["exposure_label"] + "_"
id_exposure = config["input"]["id_exposure"]
id_outcome = config["input"]["id_outcome"]
l2_dir = config["analysis"]["estimate_R"]["l2_dir"] # where LD score reference data lives

selection_type = []
if "classic_Lasso" in config["analysis"]["confounder_selection"]["method"]:
    classic_Lasso_string = expand("classic_Lasso_{lambda_type}_seed{seed}",
                              lambda_type = config["analysis"]["confounder_selection"]["lambda_type"],
                              seed = config["analysis"]["confounder_selection"]["seed"])
    selection_type.extend(classic_Lasso_string)

if "double_Lasso" in config["analysis"]["confounder_selection"]["method"]:
    double_Lasso_string = expand("double_Lasso_{lambda_type}_seed{seed}",
                              lambda_type = config["analysis"]["confounder_selection"]["lambda_type"],
                              seed = config["analysis"]["confounder_selection"]["seed"])
    selection_type.extend(double_Lasso_string)

if "stepwise" in config["analysis"]["confounder_selection"]["method"]:
    stepwise_string =  expand("stepwise_{stepwise_method}",
                              stepwise_method = config["analysis"]["confounder_selection"]["stepwise_method"])
    selection_type.extend(stepwise_string)

if "marginal" in config["analysis"]["confounder_selection"]["method"]:
    marginal_string = expand("marginal_p_{marginal_p}",
                              marginal_p = config["analysis"]["confounder_selection"]["marginal_p"])
    selection_type.extend(marginal_string)

if "corrected_Lasso" in config["analysis"]["confounder_selection"]["method"]:
    corrected_Lasso_string = expand("corrected_Lasso_{radius_type}_seed{seed}",
                                    radius_type = config["analysis"]["confounder_selection"]["radius_type"],
                                    seed = config["analysis"]["confounder_selection"]["seed"])
    selection_type.extend(corrected_Lasso_string)

if "double_corrected_Lasso" in config["analysis"]["confounder_selection"]["method"]:
    double_corrected_Lasso_string = expand("double_corrected_Lasso_{radius_type}_seed{seed}",
                                           radius_type = config["analysis"]["confounder_selection"]["radius_type"],
                                           seed = config["analysis"]["confounder_selection"]["seed"])
    selection_type.extend(double_corrected_Lasso_string)
    
if "UVMR" in config["analysis"]["confounder_selection"]["method"]:
    selection_type.append("UVMR")

MVMR_list = expand("selection_{type}",type = selection_type)
MVMR_list.append("unique_traits")

R_type = []
if "pval" in config["analysis"]["estimate_R"]["type"]:
    R_type.append("pval")
if "ldsc" in config["analysis"]["estimate_R"]["type"]:
    R_type.append("ldsc")

MVMR_method = []
if "IVW" in config["analysis"]["MVMR_analysis"]["method"]:
    IVW_string = expand("IVW_{IVW_pt}",IVW_pt = config["analysis"]["MVMR_analysis"]["p_thresh_IVW"])
    MVMR_method.extend(IVW_string)
if "GRAPPLE" in config["analysis"]["MVMR_analysis"]["method"]:
    GRAPPLE_string = expand("GRAPPLE_{GRAPPLE_pt}",GRAPPLE_pt = config["analysis"]["MVMR_analysis"]["p_thresh_GRAPPLE"])
    MVMR_method.extend(GRAPPLE_string)
if "MRBEE" in config["analysis"]["MVMR_analysis"]["method"]:
    MRBEE_string = expand("MRBEE_{MRBEE_pt}_pleio_{pleio_pt}",MRBEE_pt = config["analysis"]["MVMR_analysis"]["p_thresh_MRBEE"],
                           pleio_pt = config["analysis"]["MVMR_analysis"]["pleio_p_thresh"])
    MVMR_method.extend(MRBEE_string)
if "ESMR" in config["analysis"]["MVMR_analysis"]["method"]:
    ESMR_string = expand("ESMR_{ESMR_pt}",ESMR_pt = config["analysis"]["MVMR_analysis"]["p_thresh_ESMR"])
    MVMR_method.extend(ESMR_string)

if os.path.exists(out_dir + exposure_prefix + "qc_id_list.csv"):
    trait_id = pd.read_csv(out_dir + exposure_prefix + "qc_id_list.csv")['id'].tolist()
    
rule all:
    input:
        #data_dir + exposure_prefix + "extract_traits.RDS",
        #out_dir + exposure_prefix + "qc_id_list.csv",
        #out_dir + exposure_prefix + "qc_trait_info.csv",
        #out_dir + prefix + "downstream_filter.RDS"
        #data_dir + prefix + "download.sh",
        #data_dir + prefix + "success_download.txt"
        #data_dir + prefix + "pairwise_cor.RDS",
        #out_dir + prefix + "unique_traits.RDS"
        #expand(data_dir + prefix + "{MVMR_id}_beta.{chrom}.RDS",MVMR_id = MVMR_list,chrom = range(1,23)),
        #expand(data_dir + prefix + "{MVMR_id}_R_{type}.RDS",type = R_type, MVMR_id = MVMR_list)
        #expand(out_dir + prefix + "{MVMR_id}_MVMR_{type}.RDS",MVMR_id = MVMR_list, type=MVMR_method),
        #data_dir + prefix + "selection_instruments_Local.RDS"
        out_dir + prefix + "summary.csv",
        out_dir + prefix + "summary.png"

rule extract_traits:
    params: id_exposure = id_exposure,
            batch = config["analysis"]["extract_traits"]["batch"],
            population = config["analysis"]["pop"],
            pval_instruments = config["analysis"]["pval_instruments"],
            pval_traits = config["analysis"]["extract_traits"]["pval_traits"],
            r2_thresh = config["analysis"]["ldprune"]["r2_thresh"],
            clump_kb = config["analysis"]["ldprune"]["clump_kb"],
            min_snps = config["analysis"]["extract_traits"]["min_snps"],
            type_exposure = config["analysis"]["extract_traits"]["type_exposure"],
            type_candidate_traits = config["analysis"]["extract_traits"]["type_candidate_traits"],
            file_path = config["analysis"]["extract_traits"]["file_path"],
            ref_path = config["analysis"]["ldprune"]["ref_path"],
            df_candidate_traits = config["analysis"]["extract_traits"]["df_candidate_traits"]
    output: out = data_dir + exposure_prefix + "extract_traits.RDS"
    script: 'R/1_extract_traits.R'

checkpoint quality_control:
    input: file = data_dir + exposure_prefix + "extract_traits.RDS"
    params: nsnp_cutoff = config["analysis"]["quality_control"]["nsnp_cutoff"],
            population = config["analysis"]["quality_control"]["pop"],
            sex = config["analysis"]["quality_control"]["sex"]
    output: id_list = out_dir + exposure_prefix + "qc_id_list.csv",
            trait_info = out_dir + exposure_prefix + "qc_trait_info.csv",
    script: 'R/2_quality_control.R'

def input_downstream(wcs):
    myfile = checkpoints.quality_control.get().output[0]
    trait_id = pd.read_csv(myfile)['id'].tolist()
    extra_traits =  config["analysis"]["bidirection_mr"]["extra_trait"]
    if extra_traits == "None":
        return expand(data_dir + "bidirection_{ID1}_{ID2}.RDS", ID1 = [id_exposure,id_outcome], ID2 = trait_id)
    else:
        if extra_traits in trait_id:
            trait_id.remove(extra_traits)
    return expand(data_dir + "bidirection_{ID1}_{ID3}_{ID2}.RDS",ID1 = [id_exposure,id_outcome], ID2 = trait_id, ID3 = extra_traits)

rule extract_inst:
    params: pval_instruments = config["analysis"]["pval_instruments"],
            trait = "{ID}"
    output: out = data_dir + "inst_{ID}.RDS"
    retries: 3
    wildcard_constraints: ID = "[^_]+"
    script: "R/3_extract_inst.R"

rule extract_inst_mvmr:
    params: pval_instruments = config["analysis"]["pval_instruments"],
            trait1 = "{ID1}",
            trait2 = "{ID2}"
    output: out = data_dir + "inst_{ID1}_{ID2}.RDS"
    retries: 3
    script: "R/3_extract_inst_mvmr.R"

rule bidirection_mr:
    input: file1 = data_dir + "inst_{ID1}.RDS",
           file2 = data_dir + "inst_{ID2}.RDS"
    params: method = config["analysis"]["bidirection_mr"]["method"],
            over_dispersion = config["analysis"]["bidirection_mr"]["over_dispersion"],
            loss_function = config["analysis"]["bidirection_mr"]["loss_function"],
            min_instruments = config["analysis"]["bidirection_mr"]["min_instruments"]
    output: out = data_dir + "bidirection_{ID1}_{ID2}.RDS"
    retries: 3
    wildcard_constraints: ID1 = "[^_]+",
                          ID2 = "[^_]+"
    script: "R/3_bidirection_mr.R"

rule bidirection_mvmr:
    input: file1 = data_dir + "inst_{ID1}_{ID3}.RDS", # X/Y + M
           file2 = data_dir + "inst_{ID2}.RDS", # Z
           file3 = data_dir + "inst_{ID2}_{ID3}.RDS", # Z + M
           file4 = data_dir + "inst_{ID1}.RDS" # X/Y
    params: min_instruments = config["analysis"]["bidirection_mr"]["min_instruments"]       
    output: out = data_dir + "bidirection_{ID1}_{ID3}_{ID2}.RDS"
    retries: 3
    script: "R/3_bidirection_mvmr.R"
    
# Do both downstream filtering and delete high correlation traits with either X and Y
rule downstream_filter:
    input: id_list = out_dir + exposure_prefix + "qc_id_list.csv", 
           mr_files = input_downstream,
           trait_info = out_dir + exposure_prefix + "qc_trait_info.csv"
    params: id_exposure = id_exposure,
            sig_level = config["analysis"]["downstream_filter"]["sig_level"],
            R2_cutoff = config["analysis"]["quality_control"]["R2_cutoff"],
            extra_trait = config["analysis"]["bidirection_mr"]["extra_trait"]
    output: out = out_dir + prefix + "downstream_filter.RDS"
    script:"R/3_downstream_filter.R"

rule generate_download_file:
    input: file =  out_dir + prefix + "downstream_filter.RDS",
           df_harmonise = "/nfs/turbo/sph-jvmorr/CRP_project/pipeline/harmonised_list.txt"
    params: path = original_gwas_dir,
            checkpoint = data_dir + prefix + "success_download.txt",
            id_exposure = id_exposure,
            id_outcome = id_outcome
    output: out = data_dir + prefix + "download.sh"
    script:"R/4_download_data.R"

checkpoint download_gwas:
    input: data_dir + prefix + "download.sh"
    output: data_dir + prefix + "success_download.txt"
    shell: "bash {input}"

def input_combine_gwas(wcs):
    myfile = checkpoints.download_gwas.get().output[0]
    return out_dir + prefix + "downstream_filter.RDS"
                  
rule combine_gwas:
    input: file = input_combine_gwas,
           download = data_dir + prefix + "download.sh"
    params: path = original_gwas_dir
    output: out = data_dir + prefix + "all_beta.{chrom}.RDS"
    wildcard_constraints: chrom = "\d+"
    script:"R/4_combine_gwas.R"

rule calculate_cor:
    input: beta = expand(data_dir + prefix + "all_beta.{chrom}.RDS", chrom = range(1, 23)),
           m = expand(l2_dir + "{chrom}.l2.M_5_50", chrom = range(1, 23)),
           l2 = expand(l2_dir + "{chrom}.l2.ldscore.gz", chrom = range(1, 23))
    output: out = data_dir + prefix + "pairwise_cor.RDS"
    wildcard_constraints: pt = "[\d.]+"
    script: "R/4_ldsc_full.R"

rule unique_traits:
    input: file = out_dir + prefix + "downstream_filter.RDS",
           pairwise_cor = data_dir + prefix + "pairwise_cor.RDS"
    params: R2_cutoff = config["analysis"]["unique_traits"]["R2_cutoff"],
            method = config["analysis"]["unique_traits"]["method"],
            extra_traits =  config["analysis"]["bidirection_mr"]["extra_trait"]
    output: out = out_dir + prefix + "unique_traits.RDS"
    script: "R/4_unique_traits.R"

rule selection_instruments_api:
    input: file = out_dir + prefix + "unique_traits.RDS"
    params: id_exposure = id_exposure,
            id_outcome = id_outcome,
            r2_thresh = config["analysis"]["confounder_selection"]["r2_thresh"],
            clump_kb = config["analysis"]["confounder_selection"]["clump_kb"],
            pval_threshold = config["analysis"]["confounder_selection"]["pval_threshold"],
            find_proxies = config["analysis"]["confounder_selection"]["find_proxies"],
            population = config["analysis"]["pop"],
            harmonise_strictness = config["analysis"]["confounder_selection"]["harmonise_strictness"]
    output: out = data_dir + prefix + "selection_instruments_API.RDS"
    script: "R/5_selection_instruments_api.R"

rule selection_instruments_local:
    input: beta = expand(data_dir + prefix + "unique_traits_beta_ldpruned.{chrom}.RDS", chrom = range(1, 23))
    params: pval_threshold = config["analysis"]["confounder_selection"]["pval_threshold"]
    output: out = data_dir + prefix + "selection_instruments_Local.RDS"
    script: "R/5_selection_instruments_local.R"

instrument_file =  data_dir + prefix + "selection_instruments_" + config["analysis"]["confounder_selection"]["method_instruments"] + ".RDS"

rule classic_Lasso:
    input: file = out_dir + prefix + "unique_traits.RDS",
           instruments = instrument_file
    params: id_exposure = id_exposure,
            lambda_type = lambda wc: wc.get("lambda_type"),
            seed = config["analysis"]["confounder_selection"]["seed"]
    output: out = out_dir + prefix + "selection_classic_Lasso_{lambda_type}_seed{seed}.RDS"
    wildcard_constraints: seed = "\d+"
    script: "R/5_classic_Lasso.R"

rule double_Lasso:
    input: file = out_dir + prefix + "unique_traits.RDS",
           instruments = instrument_file
    params: id_exposure = id_exposure,
            lambda_type = lambda wc: wc.get("lambda_type"),
            seed = config["analysis"]["confounder_selection"]["seed"]
    output: out = out_dir + prefix + "selection_double_Lasso_{lambda_type}_seed{seed}.RDS"
    wildcard_constraints: seed = "\d+"
    script: "R/5_double_Lasso.R"

rule stepwise:
    input: file = out_dir + prefix + "unique_traits.RDS",
           instruments = instrument_file
    params: id_exposure = id_exposure,
            method = lambda wc: wc.get("stepwise_method")
    wildcard_constraints: stepwise_method = '[a-z]+'
    output: out = out_dir + prefix + "selection_stepwise_{stepwise_method}.RDS"
    script: "R/5_stepwise.R"

rule marginal:
    input: file = out_dir + prefix + "unique_traits.RDS",
           file_bidirection = out_dir + prefix + "downstream_filter.RDS"
    params: p_cutoff = lambda wc: wc.get("marginal_p"),
            extra_traits =  config["analysis"]["bidirection_mr"]["extra_trait"]
    output: out = out_dir + prefix + "selection_marginal_p_{marginal_p}.RDS"
    wildcard_constraints: marginal_p = "\d+(\.\d+)?"
    script: "R/5_marginal.R"

rule corrected_Lasso:
    input: file = out_dir + prefix + "unique_traits.RDS",
           instruments = instrument_file
    params: id_exposure = id_exposure,
            radius_type = lambda wc: wc.get("radius_type"),
            seed = config["analysis"]["confounder_selection"]["seed"],
            maxits = config["analysis"]["confounder_selection"]["maxits"]
    output: out = out_dir + prefix + "selection_corrected_Lasso_{radius_type}_seed{seed}.RDS"
    wildcard_constraints: seed = "\d+"
    script: "R/5_corrected_Lasso.R"

rule double_corrected_Lasso:
    input: file = out_dir + prefix + "unique_traits.RDS",
           instruments = instrument_file
    params: id_exposure = id_exposure,
            radius_type = lambda wc: wc.get("radius_type"),
            seed = config["analysis"]["confounder_selection"]["seed"],
            maxits = config["analysis"]["confounder_selection"]["maxits"]
    output: out = out_dir + prefix + "selection_double_corrected_Lasso_{radius_type}_seed{seed}.RDS"
    wildcard_constraints: seed = "\d+"
    script: "R/5_double_corrected_Lasso.R"

rule UVMR:
    output: temp(out_dir + prefix + 'UVMR.RDS')
    shell: "touch {output}"
    
def input_MVMR_combine_gwas(wildcards):
    myfile = checkpoints.download_gwas.get().output[0]
    MVMR_id = wildcards.MVMR_id
    if MVMR_id == 'selection_UVMR':
        return out_dir + prefix + 'UVMR.RDS'
    else:
        return out_dir + prefix + f"{MVMR_id}.RDS"

rule MVMR_combine_gwas:
    input: file = input_MVMR_combine_gwas,
           download = data_dir + prefix + "download.sh"
    params: path = original_gwas_dir,
            id_exposure = id_exposure,
            id_outcome = id_outcome,
    output: out = data_dir + prefix + "{MVMR_id}_beta.{chrom}.RDS"
    wildcard_constraints: chrom = "\d+",
                          MVMR_id = "(?!.*all).*"
    script:"R/6_MVMR_combine_gwas.R"

rule MVMR_LD_clumping:
    input: beta = data_dir + prefix + "{MVMR_id}_beta.{chrom}.RDS"
    params: r2_thresh = config["analysis"]["ldprune"]["r2_thresh"],
            clump_kb = config["analysis"]["ldprune"]["clump_kb"],
            ref_path = config["analysis"]["ldprune"]["ref_path"],
            ld_prioritization = config["analysis"]["ldprune"]["ld_prioritization"],
            pthresh = config["analysis"]["ldprune"]["pthresh"]
    output: out = data_dir + prefix + "{MVMR_id}_beta_ldpruned.{chrom}.RDS"
    wildcard_constraints: chrom = "\d+"
    script:"R/6_ld_prune_plink.R"

rule MVMR_R_ldsc:
    input: beta = expand(data_dir + prefix + "{{MVMR_id}}_beta.{chrom}.RDS", chrom = range(1, 23)),
           m = expand(l2_dir + "{chrom}.l2.M_5_50", chrom = range(1, 23)),
           l2 = expand(l2_dir + "{chrom}.l2.ldscore.gz", chrom = range(1, 23))
    output: out = data_dir + prefix + "{MVMR_id}_R_ldsc.RDS"
    script: "R/4_ldsc_full.R"

rule MVMR_R_pval:
    input: beta = expand(data_dir + prefix + "{{MVMR_id}}_beta_ldpruned.{chrom}.RDS", chrom = range(1, 23))
    params: p_thresh = config["analysis"]["estimate_R"]["p_thresh"]
    output: out = data_dir + prefix + "{MVMR_id}_R_pval.RDS"
    script: "R/6_estimate_R_pval.R"

rule MVMR_IVW:
    input: beta = expand(data_dir + prefix + "{{MVMR_id}}_beta_ldpruned.{chrom}.RDS", chrom = range(1, 23))
    params: pval_threshold = lambda wc: wc.get("IVW_pt"),
    output: out = out_dir + prefix + "{MVMR_id}_MVMR_IVW_{IVW_pt}.RDS"
    script: "R/7_MVMR_IVW.R"

MVMR_R_type = config["analysis"]["MVMR_analysis"]["R_type"]

rule MVMR_GRAPPLE:
    input: beta = expand(data_dir + prefix + "{{MVMR_id}}_beta_ldpruned.{chrom}.RDS", chrom = range(1, 23)),
           R = data_dir + prefix + "{MVMR_id}_R_" + MVMR_R_type + ".RDS"
    params: pval_threshold = lambda wc: wc.get("GRAPPLE_pt"),
            R_type = MVMR_R_type
    output: out = out_dir + prefix + "{MVMR_id}_MVMR_GRAPPLE_{GRAPPLE_pt}.RDS"
    script: "R/7_MVMR_GRAPPLE.R"

rule MVMR_MRBEE:
    input: beta = expand(data_dir + prefix + "{{MVMR_id}}_beta_ldpruned.{chrom}.RDS", chrom = range(1, 23)),
           R = data_dir + prefix + "{MVMR_id}_R_" + MVMR_R_type + ".RDS"
    params: pval_threshold = lambda wc: wc.get("MRBEE_pt"),
            pleio_p_thresh = lambda wc: wc.get("pleio_pt"),
            R_type = MVMR_R_type
    output: out = out_dir + prefix + "{MVMR_id}_MVMR_MRBEE_{MRBEE_pt}_pleio_{pleio_pt}.RDS"
    script: "R/7_MVMR_MRBEE.R"

rule MVMR_ESMR:
    input: beta = expand(data_dir + prefix + "{{MVMR_id}}_beta_ldpruned.{chrom}.RDS", chrom = range(1, 23)),
           R = data_dir + prefix + "{MVMR_id}_R_" + MVMR_R_type + ".RDS"
    params: pval_threshold = lambda wc: wc.get("ESMR_pt"),
            R_type = MVMR_R_type
    output: out = out_dir + prefix + "{MVMR_id}_MVMR_ESMR_{ESMR_pt}.RDS"
    script: "R/7_MVMR_ESMR.R"

rule summary_result:
    input: file = expand(out_dir + prefix + "{MVMR_id}_MVMR_{type}.RDS",MVMR_id = MVMR_list, type=MVMR_method)
    params: prefix = prefix,
            id_exposure = id_exposure,
            id_outcome = id_outcome
    output: out1 = out_dir + prefix + "summary.csv",
            out2 = out_dir + prefix + "summary.png"
    script: "R/8_summary_result.R"



