---
title: "mrScan vignette"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{mrScan vignette}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  markdown: 
    wrap: 72
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(mrScan)
library(dplyr)
library(TwoSampleMR)
library(ieugwasr)
library(hdme)
library(kableExtra)
library(ggplot2)
```

We'll use C-reactive protein (CRP) level (ebi-a-GCST90029070) as the exposure and
stroke (ebi-a-GCST005838) as the outcome in this example. Please note if
you want to conduct the whole analysis parallel, we provide a Snakemake
pipeline and you can check the usage in the README. You also need to set up 
OpenGWAS API user authentication first. You could refer the specific document
at [their website](https://api.opengwas.io/api/#authentication) and 
[ieugwasr tutorial](https://mrcieu.github.io/ieugwasr/articles/guide.html#authentication).

```{r}
x_id <- "ebi-a-GCST90029070"
y_id <- "ebi-a-GCST005838"
```

## Step 1: Initially extract trait list

In this step, we will do phenome-wide scan to get an initial confounder 
list. For user's convenience, we provide options of local or OpenGWAS data for 
both exposure and candidate confounders. To be specific, you can choose one option 
according to your needs:

1. You don't have local GWAS summary data now, or you only want to have a quick 
look about the confounder list. Then you only need to input the GWAS ID of the exposure.  

In this example, we only search traits in ieu-a and ieu-b batch (GWAS summary datasets 
generated by many different consortia that have been manually collected and curated, 
initially developed for MR-Base) in IEU OpenGWAS database. You can add more batches 
according to your needs. You can check the list of data batches in IEU GWAS database 
by ieugwasr::batches(). You can also change the trait search parameters including 
pvalues, population and clumping options.  

Note: this function is based on `ieugwasr::phewas()`, which could cause API error 
when using subset batches. You can also try `options(ieugwasr_api = 'gwas-api.mrcieu.ac.uk/')`
and rerun. 

```{r eval=FALSE}
res_step1 <- extract_traits(id_exposure = x_id,batch = c("ieu-a","ieu-b","ukb-b"))
```

In this analysis, we get 209 traits having at least 5 shared variants
with the CRP-level.
```{r}
load("data/res_step1.rda")
length(res_step1$id.list)
head(res_step1$trait.info)
```

2. You already have a list of candidate traits and don't want to search
traits in IEU OpenGWAS database, you can download GWAS data locally. You need
to specify the file path for each trait in `file_list` and trait ID in
`trait_list`. Then check the data to find column names of `SNP` (rsid),
`beta_hat`, `se`, `p_value` and input in the function like:

```{r eval=FALSE}
extract_traits(id_exposure = x_id,type_candidate_traits = "local",
               file_list = c("/path/to/trait1","/path/to/trait2"),
               trait_list = c("trait1","trait2"),
               snp_name_list = c("snp_name1","snp_name2"),
               beta_hat_name_list = c("beta_hat_name1","beta_hat_name2"),
               se_name_list = c("se_name1","se_name2"),
               p_value_name_list = c("pval_name1","pval_name2"))
```

Note the `p_value_name_list` could be NA if the data does not have
pvalue. If the local files are downloaded from IEU Open GWAS database or
EBI database (harmonized one), you can just omit these parameters and
don't need to check the exact column names. But you must need to provide
snp, beta_hat and se column names if these data is from other sources.  


You can also create a candidate trait info dataframe to specify all information. 
(Please check the package README, Optional Step, for detailed
instruction). The example candidate info is like:

```{r}
df_candidate_info <- read.csv("data/example_candidate_info.csv")
df_candidate_info
```


After generating the candidate info dataframe, you can run code by:

```{r eval=FALSE}
extract_traits(id_exposure = x_id,type_candidate_traits = "local",
               file_list = df_candidate_info$path,
               trait_list = df_candidate_info$trait_ID,
               snp_name_list = df_candidate_info$snp,
               beta_hat_name_list = df_candidate_info$beta_hat,
               se_name_list = df_candidate_info$se,
               p_value_name_list = df_candidate_info$p_value)
```

3. You want to use your local data for the main exposure, but still use OpenGWAS 
database to search candidate confounders. Then you need to set 
`type_exposure="local"`, and input data path in `file_path`. You need to
download the LD reference files [here](http://fileserve.mrcieu.ac.uk/ld/1kg.v3.tgz) 
to perform LD clumping. And then input `ref_path` to your directory path of LD
reference file. The code is like:

```{r eval=FALSE}
extract_traits(id_exposure = x_id,type_exposure = "local",
               file_path = "/path/to/exposure_file.vcf.gz",
               ref_path = "/path/to/reference/EUR",
               batch = c("ieu-a","ieu-b"))
```

4. You want to use local data for the main exposure, and also local data for the 
candidate confounder list. Just combine the code of situation 2 and 3 like:

```{r eval=FALSE}
extract_traits(id_exposure = x_id,
               type_exposure = "local",type_candidate_traits = "local",
               file_path = "/path/to/exposure_file.vcf.gz",
               ref_path = "/path/to/reference/EUR",
               file_list = df_candidate_info$path,
               trait_list = df_candidate_info$trait_ID,
               snp_name_list = df_candidate_info$snp,
               beta_hat_name_list = df_candidate_info$beta_hat,
               se_name_list = df_candidate_info$se,
               p_value_name_list = df_candidate_info$p_value)
```


## Step 2: Quality control

In this step, we'll do quality control in initial filtering traits. We select traits
for both gender, European and with the number of SNPs \> 1e6. This function will also 
delete traits which have been adjusted for other factors like sex, BMI. You can change the 
filtering standard for your need. You can check the info matrix for the status of each trait. 
We get 181 traits after quality control filtering. 

```{r}
res_step2 <- quality_control(dat = res_step1$trait.info)
length(res_step2$id.list)
res_step2$trait.info %>% filter(status == "delete in QC")
```

Please make sure you go through the trait info for this step. The filtering step is 
only based on uploaded GWAS study information, and sometimes the info could be missing. 
After checking, you can manually delete or add back traits according to your needs.  


You can find several duplicated traits here, so we will do an initial duplicates 
filtering. This is based on string similarity score calculated by Jaro and Winkler algorithm 
between trait names from the metadata. We recommend to make the `R2_cutoff` pretty 
high to make sure we have a stringent filtering to avoid deleting important traits. 
You can omit this step if your confounder list is not quite long, since we'll do 
more accurate duplicate filtering in the following steps.

```{r warning=FALSE}
all_inst <- extract_instruments(c(x_id,y_id,res_step2$id.list))
df_inst_counts <- data.frame(id = res_step2$id.list, n_inst = NA)
for (i in df_inst_counts$id) {
  inst <- all_inst %>% filter(id.exposure == i)
  num_instruments <- nrow(inst)
  df_inst_counts <- df_inst_counts %>% 
    mutate(n_inst = ifelse(id == i, num_instruments, n_inst))
}
```


We get 127 traits after initial filtering.
```{r warning=FALSE}
res_step2_filter <- string_filter(id.list = res_step2$id.list, df_info = res_step2$trait.info,
                                  df_inst_counts = df_inst_counts,
                                  R2_cutoff = 0.95)
length(res_step2_filter$id.list)
```


## Step 3: Downstream traits filtering

Downstream traits of both main exposure and each outcome should be
deleted since the precision for estimating direct causal effect of the
main exposure to the outcome could be decreased if too many genetic
instruments of relationship between the main exposure and downstream
traits were included. In this step, we will do bidirection MR estimates
between traits and X/Y and then select all upstream traits and exclude
downstream traits in this step.

### Bidirection MR

First of all, we'll do simple MR between each candidate traits ($Z$) and
either the main exposure and the outcome. To be specific, we will do
four MR estimates including: $X \rightarrow Z$, $Z \rightarrow X$,
$Y \rightarrow Z$ and $Z \rightarrow Y$. Note that this step may take some time. 
We recommend to extract instruments for all traits first to avoid repeating extracting
instruments. 

The bidirection MR step can filter outliers by standardized effect size threshold 
(`effect_size_cutoff`), and it also do steiger filtering to filter instruments. 
You could refer `TwoSampleMR::steiger_filtering()` for details. In steiger filtering,
you need to specify trait type (continuous or binary), and disease prevalence for 
binary traits. At this example, we assume all candidate confounders are continuous traits, 
but assign the outcome as binary trait since it's a disease outcome. We use 0.01 as 
the approximate prevalence of stroke. We want to delete traits with limited number of instruments.
If you use MR_GRAPPLE, set `min_instrument = 8`.

```{r}
df_pair <- expand.grid(trait1 = c(x_id,y_id), trait2 = res_step2_filter$id.list)
```


```{r eval=FALSE}
res_mr <- purrr::map2(df_pair$trait1, df_pair$trait2, function(i, j){
  ex_dat1 <- all_inst %>% filter(id.exposure == i)
  ex_dat2 <- all_inst %>% filter(id.exposure == j)
  if(unique(ex_dat1$id.exposure) ==  x_id){
    bidirection_mr(ex_dat1 = ex_dat1, ex_dat2 = ex_dat2,min_instruments=8)
  }else{
    bidirection_mr(ex_dat1 = ex_dat1, ex_dat2 = ex_dat2,
                   type_list = c("binary","continuous"),
                   prevalence_list = list(0.01, NULL),
                   min_instruments=8)
  }
})
```


The `bidirection_mr` will give MR estimates by three default MR methods including 
MR_IVW, MR_GRAPPLE, and MR_MRBEE. It also record the instrument data correlation, 
and outlier SNPs. 

```{r}
load("data/res_mr.rda")
names(res_mr) <- as.vector(df_pair$trait2)
res_mr[[1]]
```

We will delete traits with limited number of instruments. 
```{r}
empty_id <- df_pair[which(sapply(res_mr,is.null)),"trait2"] %>% unique() %>% as.vector()
res_step2_filter$trait.info[res_step2_filter$trait.info$id %in% empty_id,
                            "status"] <- "delete due to not enough instruments"
res_step2_filter$id.list <- res_step2_filter$id.list[!res_step2_filter$id.list %in% empty_id]
```

We have calculated the coarse genetic correlation between each candidate
trait and either X and Y when doing bidirection MR by calculating the
beta_hat correlation for genetic instruments. This is not 100% accurate
but may enough for filtering high correlation traits with X and Y. We
still suggest you to check the result. Here, we will delete traits with high correlation 
for X or Y.

```{r}
complete_res_mr <- Filter(Negate(is.null), res_mr)
df_pair_cor <- do.call(rbind, lapply(complete_res_mr, function(x) x$cor))
high_corr_id <- df_pair_cor %>% filter(cor > 0.85) %>% pull(id2) 
res_step2_filter$trait.info[res_step2_filter$trait.info$id %in% high_corr_id,
                            "status"] <- "delete since high cor with X or Y"
res_step2_filter$id.list <- res_step2_filter$id.list[!res_step2_filter$id.list %in% high_corr_id]
```


### (Optional Step) Bidirection MVMR

Sometimes if you have background knowledge about this relationship, you
may already have certain heritable confounders. Instead of simple
univariable MR, you can adjust for the specific confounder ($M$) in the
bidirection estimate step. Then you will do four MVMR estimates
including: $X + M \rightarrow Z$, $Z + M\rightarrow X$,
$Y + M \rightarrow Z$ and $Z + M \rightarrow Y$. For instance, in this
CRP-level and stroke example, we may know BMI could be an
important confounder, so we can adjust for a BMI trait (ukb-b-19953).

```{r eval=FALSE}
m_id <- "ukb-b-19953"
new_id <- res_step2_filter$id.list[!res_step2_filter$id.list %in% m_id]
df_pair <- expand.grid(trait1 = c(x_id,y_id), trait2 = new_id)
mvmr_pair <- mapply(append, c(x_id,y_id,new_id), m_id, SIMPLIFY = FALSE)
```


```{r eval=FALSE}
all_inst_mvmr <- lapply(mvmr_pair,mv_extract_exposures)
res_mvmr <- purrr::map2(df_pair$trait1, df_pair$trait2, function(i, j){
  ex_dat1 <- all_inst_mvmr[[which(names(mvmr_pair) == i)]] # (X/Y + M)
  ex_dat2 <- all_inst %>% filter(id.exposure == j) # Z
  ex_dat3 <- all_inst_mvmr[[which(names(mvmr_pair) == j)]] # (Z + M)
  ex_dat4 <- all_inst %>% filter(id.exposure == i) # X/Y
  if(unique(ex_dat4$id.exposure) == x_id){
    type_list <- c("continuous","continuous")
    prevalence_list <- c(NA, NA)
}else{
  type_list <- c("binary","continuous")
  prevalence_list <- c(0.01, NA)
}
  bidirection_mvmr(ex_dat1 = ex_dat1, ex_dat2 = ex_dat2,
                   ex_dat3 = ex_dat3, ex_dat4 = ex_dat4,
                   type_list = type_list, prevalence_list = prevalence_list,
                   min_instruments = 8)
})
```

### Downstream filtering

In this step, we'll select all upstream traits and exclude downstream traits for 
either the main exposure and the outcome. We'll estimate FDR for p-values obtained 
from previous bidirection MR analysis, and then retain traits with an estimated FDR 
below cutoff for their effect on either X or Y, and excluded any that exhibited 
an estimated FDR below cutoff for effects from the exposure or outcome. You need to 
specify one MR method for filtering, here we use MR_GRAPPLE as the example. After this 
step, we select 36 potential confounders. 


```{r}
res_mr_filter <- complete_res_mr[which(!names(complete_res_mr) %in% high_corr_id)]
names(res_mr_filter) <- NULL
all_mr <- do.call(Map, c(f = rbind, res_mr_filter))
res_step3 <- downstream_filter(id_exposure = x_id,id.list = res_step2_filter$id.list,
                               df_info = res_step2_filter$trait.info, res = all_mr,
                               MR_method = "MR_GRAPPLE")
length(res_step3$id.list)
data.frame(res_step3$df_bidirection) %>% head()
```


## Step 4: Get unique traits

By the trait list above, we can see several similar or even duplicated
traits are extracted. In this step, we'll calculate genetic correlation
for each trait pair and select unique traits.

There are two ways to get trait correlation:   

1. Simply calculate string similarity by Jaro-Winkler distance. This way is quick 
but biased because it only based on provided trait names. If you want to quickly
check it by this method, we recommend you to check the df_pair matrix
and the cluster result.  

```{r}
res_string_sim <- string_sim(id.list = res_step3$id.list, df_info = res_step3$trait.info)
```

2. Calculate accurate genetic correlation by LDSC method. It will need
you to download GWAS summary data for traits locally and we recommend you to do 
it on a cluster server. If your current confounder list is quite long (more than 30 traits),
please refer to the Snakemake pipeline to conduct jobs parallelly. Here we provide 
steps to download data, format and harmonize data, and calculate genetic correlation matrix. 

### Download GWAS summary statistics

You can use the following code to generate download.sh file and conduct
it in the linux command line. 

This function has a few options you may need to change:  

- `df_harmonise`: If all traits you want to download are from IEU OpenGWAS 
database, just leave it as `NULL`. If you want to include traits from 
EBI database, please download `harmonised_list.txt` first [here](https://ftp.ebi.ac.uk/pub/databases/gwas/summary_statistics/harmonised_list.txt).
And then change this parameter to the file path. In this example, the outcome data 
is from EBI database, so we set the file in the below. 

- `data_path`: The directory you want to download the data.

- `path_checkpoint`: The path of successfully download `.txt` file. If all data 
has been successfully downloaded, it will generate a checkpoint file.

```{r eval=FALSE}
df_harmonise <- read.csv("data/harmonised_list.txt",header = F)
df_download <- download_gwas(id_list = c(x_id,y_id,res_step3$id.list),
                             df_harmonise = df_harmonise,
                             data_path = "data/",
                             path_checkpoint = "data/success_download.txt")
write.table(df_download,file = "data/download.sh",row.names = FALSE,
            col.names = FALSE, quote = FALSE)
```

Then conduct this code in the terminal command line.
```{bash, eval=FALSE, engine="sh"}
bash data/download.sh
```

### Format and harmonize data

After downloading data, next step is to format raw GWAS summary data to
remove ambiguous SNPs and then combine them together. You need to generate a 
dataframe contain `id` (trait id) and `location` (data location) columns first.
This function will be conducted for every chromosome.  


In order to run the code parallel, we also prepare an example R script `combine_gwas.R` and bash file `combine_gwas.sh` for submitting job on servers. You can refer to that and edit 
according to your needs.

```{r eval=FALSE}
df_file <- data.frame(id = res_step3$id.list) %>%
  mutate(location = paste0("data/",id,".vcf.gz"))
for (c in 1:22) {
  dat <- format_combine_gwas(df_file = df_file, c = c, 
                             df_info = res_step3$trait.info)
  saveRDS(dat,file = paste0("data/all_beta.",c,".RDS"))
}
```

### Calculate R correlation matrix

There are two options for this. One is by using p-value method and the
other is LDSC way. p-value method is quick and easier but the estimated
matrix could be biased. If you want to use more accurate LDSC way, you
need to [download](https://zenodo.org/records/8182036) reference LD scores from 
1000 Genomes first. You can conduct the code like:

```{r eval=FALSE}
# pvalue method
R_est_pval <- estimate_R_pval(beta_files = paste0("data/all_beta.",seq(1,22),".RDS"))
```


```{r eval=FALSE}
# LDSC method
R_est_ldsc <- ldsc_full(dat = paste0("data/all_beta.",seq(1,22),".RDS"), 
                        ld_files = paste0("/path/eur_w_ld_chr/",seq(1,22),".l2.ldscore.gz"), 
                        m_files = paste0("/path/eur_w_ld_chr/",seq(1,22),".l2.M_5_50"))
```


```{r}
R_est_ldsc <- readRDS("data/R_est_ldsc.RDS")
abs(R_est_ldsc$Rg) %>% head()
```

Finally, we'll make highly correlated traits cluster together by pairwise correlation 
matrix. The default clustering method is greedy clustering on the filtering by the 
`R^2` cutoff. You may tune the cutoff according to your needs. 
You can also choose sample_size or nsnps method, which are basically selected traits 
with higher sample size or the number of SNPs.  

After clustering, we'll choose one representative trait in each cluster. The selection 
method could be either `n_inst` (select traits with the largest number of instruments), 
or `pvalue` (select traits with the largest value on the mean of $-log(FDR_{ZtoX})-log(FDR_{ZtoY})$). 
If you use `bidirection_mvmr` before, please input the confounder trait at the `extra_traits` parameter. 

```{r}
Rg <- abs(R_est_ldsc$Rg)
df_pairs <- reshape2::melt(Rg, value.name = "cor",varnames = c("id1","id2")) %>%
  filter(cor != 1)
df_matrix <- data.frame(Rg,check.names = FALSE)
res_step4 <- unique_traits(id.list = res_step3$id.list, df_info = res_step3$trait.info, 
                           R_matrix = df_matrix, df_pairs = df_pairs, 
                           R2_cutoff = 0.8)
```

We get 21 unique traits after this step:
```{r}
res_step4$trait.info %>% filter(id %in% res_step3$id.list) %>% 
  select(id,trait,cluster,n_inst) %>% arrange(cluster)
length(res_step4$id.list)
```


## Step 5: Instrument strength filtering

We want to make sure every trait has enough instrument strength to eliminate weak 
instrument bias. In this step, we will calculate conditional F-statistics of the 
instrument strength for each trait, and then filter out traits with low F-stats.

### Extract instrument data

There are two options to get instruments after LD clumping.

1. Through OpenGWAS API, which is based on TwoSampleMR package. It will work when 
you don't have a large trait list and you want to quickly check the strength.

```{r eval=FALSE}
mv_exposure_dat <- mv_extract_exposures(c(x_id,res_step4$id.list))
mv_outcome_dat <- extract_outcome_data(mv_exposure_dat$SNP, y_id)
mvdat <- mv_harmonise_data(mv_exposure_dat,mv_outcome_dat)
```


2. Through local data. It needs to combine and format data, calculate pairwise sample 
overlap, and do LD clumping. The previous two steps have already been introduced before. 
Note that this time we need to include the main exposure and outcome, you should 
**put the outcome as the first trait**.

```{r eval=FALSE}
id_list_new <- c(y_id,x_id,res_step4$id.list)
file_list <- df_download$V1 %>% strsplit("/") %>% sapply(tail,1) %>% head(-1) %>%
  data.frame() %>% setNames("location") %>% filter(!str_detect(location, "\\.tbi$")) %>%
  mutate(id = case_when(
    str_detect(location, "\\.vcf\\.gz$") ~ gsub("\\.vcf\\.gz$", "", location),
    str_detect(location, "-GCST.*\\.h\\.tsv\\.gz$") ~ paste0("ebi-a-", str_extract(location, "GCST[0-9]+"))
  )) %>% filter(id %in% id_list_new)
file_list$location <- paste0("data/",file_list$location)
df_unique <- data.frame(id=id_list_new) %>% left_join(file_list)
for (c in 1:22) {
  dat <- format_combine_gwas(df_file = df_unique, c = c, 
                             df_info = res_step4$trait.info)
  saveRDS(dat,file = paste0("data/unique_traits_beta.",c,".RDS"))
}
# Calculate genetic correlation
R_ldsc_unique <- ldsc_full(dat = paste0("data/unique_traits_beta.",seq(1,22),".RDS"), 
                           ld_files = paste0("/path/eur_w_ld_chr/",seq(1,22),".l2.ldscore.gz"), 
                           m_files = paste0("/path/eur_w_ld_chr/",seq(1,22),".l2.M_5_50"))
```

Here is the sample overlap matrix calculated by LDSC:
```{r}
R_ldsc_unique <- readRDS("data/R_ldsc_unique.RDS")
as.matrix(R_ldsc_unique$Re) %>% head()
```

Then we will do LD clumping for local data.  You can download the LD reference dataset from [here](http://fileserve.mrcieu.ac.uk/ld/1kg.v3.tgz). This contains an LD reference 
panel for each of the 5 super-populations in the 1000 genomes reference dataset. 

```{r eval=FALSE}
for (c in 1:22) {
  dat_unique_ldpruned <- ld_prune_plink(X = readRDS(paste0("data/unique_traits_beta.",c,".RDS")),
                                        ref_path = "/path/1kg_plink/EUR")
  saveRDS(dat_ldpruned, file = paste0("data/unique_traits_beta_ldpruned.",c,".RDS"))
}
```

### Calculate instrument strength and filter traits

The default F-stats cutoff is 5, and you can change it to 10 if you want a more 
stringent threshold. If you want to include some traits no matter the instrument 
strength, set the `extra_traits` option to avoid filtering them out.  


1. If you use data by API, you can assume there is no sample overlap here, 
but the results could be biased. Here we can see all traits with F-stats more than 5,
so all of them will be selected.
```{r eval=FALSE}
res_strength_IEU <- strength_filter(dat = mvdat,dat_type = "IEU",
                                    df_info = res_step4$trait.info,
                                    type_outcome = "binary",prevalence_outcome = 0.01,
                                    Filter = TRUE)
res_strength_IEU$df_strength
```

2. If you use local data, combine LD pruned data first.

```{r eval=FALSE}
beta_files_unique <- paste0("data/unique_traits_beta_ldpruned.",seq(1,22),".RDS")
dat_unique <- purrr::map_dfr(beta_files_unique, readRDS)
```

Using local data with sample overlap matrix, we still see all traits having F-stats 
more than 5.
```{r}
load("data/dat_unique.rda")
res_step5 <- strength_filter(dat = dat_unique,dat_type = "local",
                             R_matrix = as.matrix(R_ldsc_unique$Re),
                             df_info = res_step4$trait.info,
                             type_outcome = "binary",prevalence_outcome = 0.01,
                             Filter = TRUE)
res_step5$df_strength
```

**Note:** If you have a long list of candidate confounders, the instrument strength 
might be quite low for some traits. This issue arises because using complete data 
for all traits can lead to the loss of instruments when combining data. In that case, 
we recommend using a stepwise filtering strategy: begin by removing traits with 
the lowest F-statistics, then re-combine and re-format the remaining traits to 
recalculate the F-statistics. Repeat this process until all traits have sufficient 
instrument strength.

## (Option step) Use other methods to do confounder selection

If until this step, your confounder list is still quite long and you 
want to further filter it, you could try the following methods. We provide 
multiple choices and you can choose methods based on your needs. 
In this example, we don't need to do it.

1. Marginal selection. It will select traits with $FDR_{ZtoX} < cutoff$ and 
$FDR_{ZtoY} < cutoff$. The default cutoff is 0.05. 

```{r}
res_marginal <- marginal(id.list = res_step5$id.list,df_info = res_step5$trait.info,
                         df_bidirection = res_step3$df_bidirection)
res_marginal$id.list
```

2. Literature information. You probably have background knowledge about the potential 
confounders. For instance, we may want to include BMI in our current list. 

```{r eval=FALSE}
literature_traits <- "ukb-b-19953"
id_literature <- unique(c(res_step5$id.list,literature_traits))
if(sum(!literature_traits %in% res_step5$trait.info$id) != 0){
  new_id <- literature_traits[!literature_traits %in% res_step5$trait.info$id]
  new_info <- res_step5$trait.info %>% full_join(gwasinfo(new_id))
  new_info[new_info$id %in% id_literature,"status"] <- "Select by literature"
  new_info %>% filter(status == "Select by literature")
}else{
  res_step5$trait.info[res_step5$trait.info %in% id_literature,"status"] <- "Select by literature"
}
```

3. Classic Lasso method. It will not penalize the main exposure. You can choose 
`lambda_type` in either `1se` or `min`, and you may set seed to make the results 
consistent. Here we input local data after strength_filter to make sure traits with 
enough instrument strength.

```{r}
load("data/dat_unique_filter.rda")
res_Lasso_min <- classic_Lasso(dat = dat_unique_filter, type = "local",id_exposure = x_id, 
                               df_info = res_step5$trait.info,
                               type_outcome = "binary", prevalence_outcome = 0.01,
                               lambda_type = "min")
res_Lasso_min$id.list
```

If you want to use data from API (results from `TwoSampleMR::mv_harmonise_data()`),
please change the type to `"IEU"`, and use code like:
```{r eval=FALSE}
classic_Lasso(dat = mvdat, type = "IEU", id_exposure = x_id, 
              df_info = res_step5$trait.info,
              type_outcome = "binary", prevalence_outcome = 0.01,
              lambda_type = "min")
```

4. Corrected Lasso method. It is based on [`hdme`](https://github.com/osorensen/hdme/tree/master) 
package. It will correct for measurement error in the Lasso for linear regression. 
We conduct this method in the genetic data since GWAS summary statistics have 
measurement error inside.
```{r eval=FALSE}
res_corrected_Lasso <- corrected_Lasso(dat = dat_unique_filter, type = "local", 
                                       id_exposure = x_id,df_info = res_step5$trait.info,
                                       type_outcome = "binary", prevalence_outcome = 0.01,
                                       radius_type="min")
```

5. Stepwise selection can support forward selection, backward selection and both direction 
selection. You can change the method to check the results.
```{r eval=FALSE}
res_stepwise <- stepwise(dat = dat_unique_filter, type = "local", 
                         id_exposure = x_id,df_info = res_step5$trait.info,
                         type_outcome = "binary", prevalence_outcome = 0.01,
                         method = "forward")
res_stepwise$id.list
```


## Step 6: MVMR analysis to get causal estimates

Finally, after adjusting for selected traits, we will apply different
MVMR methods to get direct causal estimates from the main exposure to the
outcome. In order to get more accurate result, we need to have sample overlap matrix 
between measurement errors for the outcome and all exposures. If the list 
of selected traits is pretty large, we recommend you to use the Snakemake pipeline 
and run it on the cluster server.  

Apart from basic MVMR-IVW, you can use different robust MVMR methods including 
GRAPPLE, MRBEE, and ESMR. If you have too many traits, we don't recommend you to 
use GRAPPLE since it cannot converge and give you biased estimates. You may also 
need to check the p-value thresholds of instrument selection for every method. 
If you try to run ESMR, use `R_ldsc_unique$Re_esmr` matrix as the `R_matrix`.

```{r error=FALSE, message=FALSE, warning=FALSE, results='hide'}
R_ldsc_unique_filter <- readRDS("data/R_ldsc_unique_filter.RDS")
res_IVW <- MVMR_IVW(dat = dat_unique_filter, type = "local", 
                    type_outcome = "binary",prevalence_outcome = 0.01)
res_GRAPPLE <- MVMR_GRAPPLE(dat = dat_unique_filter, type = "local", 
                            type_outcome = "binary",prevalence_outcome = 0.01,
                            R_matrix = as.matrix(R_ldsc_unique_filter$Re),
                            pval_threshold = 5e-8)
res_MRBEE <- MVMR_MRBEE(dat = dat_unique_filter, type = "local", 
                        type_outcome = "binary",prevalence_outcome = 0.01,
                        R_matrix = as.matrix(R_ldsc_unique_filter$Re))
res_ESMR <- MVMR_ESMR(dat = dat_unique_filter, 
                      type_outcome = "binary",prevalence_outcome = 0.01,
                      R_matrix = as.matrix(R_ldsc_unique_filter$Re_esmr))
```

You can combine results and plot them out:
```{r fig.width=10, fig.height=10}
df_summary <- bind_rows(res_IVW$res.summary,res_GRAPPLE$res.summary,
                        res_MRBEE$res.summary,res_ESMR$res.summary) %>% 
  mutate(CI_lower=b-qnorm(0.975)*se, CI_higher=b + qnorm(0.975)*se) %>%
  mutate(odds=exp(b),CI_lower=exp(CI_lower),CI_higher=exp(CI_higher))
selection_method <- "Unique traits filter"
df_summary %>% filter(exposure==x_id) %>%
  ggplot() + 
  geom_vline(xintercept = 1) +
  geom_point(aes(y = selection_method, x = odds, color = method,group = method),
             position=position_dodge(width = 0.9), size = 3) +
  geom_errorbar(aes(y = selection_method, xmin =CI_lower, xmax = CI_higher, color = method),
                position=position_dodge(width = 0.9)) +
  xlab("Odds Ratio (95% CI)") + coord_flip() +
  theme_bw() + ggtitle(paste0("Direct causal effect of ",x_id,"  on ",y_id))+
  theme(axis.text.y = element_text(size = 20),
        axis.text.x = element_text(size = 20, angle = 0),
        strip.text = element_text(size = 20),
        legend.text = element_text(size = 20),
        plot.title = element_text(size= 20),
        legend.title = element_blank(),
        axis.title.x = element_blank(),
        axis.title.y = element_text(size = 20),
        legend.position = "bottom")
```

You can check the instrument strength of selected confounders:
```{r}
df_strength_filter <- strength_filter(dat = dat_unique_filter, dat_type = "local",
                                      R_matrix = as.matrix(R_ldsc_unique_filter$Re),
                                      df_info = res_step5$trait.info,
                                      type_outcome = "binary",prevalence_outcome = 0.01,
                                      Filter = FALSE)
df_strength_filter %>% mutate(F_stats = round(F.statistic,2)) %>% 
  select(id,trait,F_stats) %>% 
  kbl(caption = "Selected confounders with F-statistics") %>%
  kable_classic(full_width = T, html_font = "Cambria")
```

You can check estimate results and trait information table:
```{r}
df_summary %>%
  kbl(caption = "Result Summary Table") %>%
  kable_classic(full_width = T, html_font = "Cambria")
res_step5$trait.info %>% head() %>%
  kbl(caption = "Partial Trait Information Table") %>%
  kable_classic(full_width = T, html_font = "Cambria")
```

